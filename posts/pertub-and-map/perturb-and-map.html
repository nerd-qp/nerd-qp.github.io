<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Perturb and MAP
| Joy Yang</title><link rel=stylesheet href=https://326623.github.io/css/site.min.df3bdd3442672aa286cd5adae1e1ed37de102ef4794aee82a58eae661744c9989d496ba198f45d915b2d383ca2fd6d786a417e2a7cefcafa410c1789214d47d7.css integrity="sha512-3zvdNEJnKqKGzVra4eHtN94QLvR5Su6CpY6uZhdEyZidSWuhmPRdkVstODyi/W14akF+KnzvyvpBDBeJIU1H1w=="><link rel=canonical href=https://326623.github.io/posts/pertub-and-map/perturb-and-map.html><link rel=alternate type=application/rss+xml href=https://326623.github.io/index.xml title=JoyComputing><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><meta name=author content="Joy Yang"><meta name=description content="No title  body { text-align: justify } h5 { display: inline; padding-right: 1em } h6 { display: inline; padding-right: 1em } table { border-collapse: collapse } td { padding: 0.2em; vertical-align: baseline } dt { float: left; min-width: 1.75em; text-align: right; padding-right: 0.75em; font-weight: bold; } dd { margin-left: 2.5em; } .subsup { display: inline; vertical-align: -0.2em } .subsup td { padding: 0px; text-align: left} .fraction { display: inline; vertical-align: -0."><meta property="og:title" content="Perturb and MAP"><meta property="og:description" content="No title  body { text-align: justify } h5 { display: inline; padding-right: 1em } h6 { display: inline; padding-right: 1em } table { border-collapse: collapse } td { padding: 0.2em; vertical-align: baseline } dt { float: left; min-width: 1.75em; text-align: right; padding-right: 0.75em; font-weight: bold; } dd { margin-left: 2.5em; } .subsup { display: inline; vertical-align: -0.2em } .subsup td { padding: 0px; text-align: left} .fraction { display: inline; vertical-align: -0."><meta property="og:type" content="article"><meta property="og:url" content="https://326623.github.io/posts/pertub-and-map/perturb-and-map.html"><meta property="article:published_time" content="2020-03-04T00:00:00+00:00"><meta property="article:modified_time" content="2020-03-04T00:00:00+00:00"></head><body><nav class="navbar is-transparent" role=navigation aria-label="main navigation"><div class=navbar-brand><a class=navbar-item href=https://326623.github.io/><figure class=image><img alt class=is-rounded src=https://326623.github.io/hhkb_red_esc.jpg></figure></a><a class=navbar-item href=https://326623.github.io/>JoyComputing</a></div><div class=navbar-menu><div class=navbar-start></div><div class=navbar-end><a class=navbar-item href=https://twitter.com/JoyYang11209479 rel=noopener target=_blank><span class=icon><img alt=icons/svg/twitter.svg src=https://326623.github.io/icons/svg/twitter.svg></span></a>
<a class=navbar-item href=https://www.facebook.com/joy.yang.1690671 rel=noopener target=_blank><span class=icon><img alt=icons/svg/facebook.svg src=https://326623.github.io/icons/svg/facebook.svg></span></a>
<a class=navbar-item href=mailto:yangqp5[%20AT%20]outlook.com target=_blank><span class=icon><img alt=email src=https://326623.github.io/icons/svg/email.svg></span></a>
<a class=navbar-item href=https://326623.github.io/index.xml target=_blank><span class=icon><img alt=rss src=https://326623.github.io/icons/svg/rss.svg></span></a></div></div></nav><section class="hero is-small is-info is-fullwidth"><div class=hero-body><div class=container><h1 class=title>Perturb and MAP</h1><h2 class=subtitle><time datetime=2020-03-04T00:00:00Z>March 04, 2020</time></h2></div></div></section><section class=section><div class=container><div class="content is-medium"><!doctype html><html xmlns=http://www.w3.org/1999/xhtml xmlns:x=http://www.texmacs.org/2002/extensions xmlns:m=http://www.w3.org/1998/Math/MathML><head><title>No title</title><meta content="TeXmacs 1.99.12" name=generator></meta><style type=text/css>body{text-align:justify}h5{display:inline;padding-right:1em}h6{display:inline;padding-right:1em}table{border-collapse:collapse}td{padding:.2em;vertical-align:baseline}dt{float:left;min-width:1.75em;text-align:right;padding-right:.75em;font-weight:700}dd{margin-left:2.5em}.subsup{display:inline;vertical-align:-.2em}.subsup td{padding:0;text-align:left}.fraction{display:inline;vertical-align:-.8em}.fraction td{padding:0;text-align:center}.wide{position:relative;margin-left:-.4em}.accent{position:relative;margin-left:-.4em;top:-.1em}.title-block{width:100%;text-align:center}.title-block p{margin:0}.compact-block p{margin-top:0;margin-bottom:0}.left-tab{text-align:left}.center-tab{text-align:center}.balloon-anchor{border-bottom:1px dotted #000;outline:none;cursor:help;position:relative}.balloon-anchor [hidden]{margin-left:-999em;position:absolute;display:none}.balloon-anchor: hover [hidden]{position:absolute;left:1em;top:2em;z-index:99;margin-left:0;width:500px;display:inline-block}.balloon-body{}.ornament{border-width:1px;border-style:solid;border-color:#000;display:inline-block;padding:.2em}.right-tab{float:right;position:relative;top:-1em}</style><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" language=javascript></script></head><body><h2 id=auto-1>1<span style=margin-left:1em></span>Introduction<span style=margin-left:1em></span></h2><p>Perturbation and Maximum A-Posteriori (Perturb-and-MAP) is ubiquitous in
the field of statistic learning, derived from the wonderful property of
Gumbel distribution, which gives the connection between maximization of
a series of random perturbated variables and summation. It is often used
to tackle the problem of computational intractable summation of the form
\(Z = \sum_{y \in Y} \exp (\phi (y))\), where \(Y = Y_1 \times Y_2
\times \cdots \times Y_n\). To show its intractability, simply take all
as \(Y_i = \{ 0, 1 \}\) for \(i = 1, 2, \ldots, n\), and the sum (or
partition function) thus becomes:</p><center>\(\displaystyle Z = \sum_{y_1 \in Y_1, y_2 \in Y_2, \ldots, y_n \in Y_n}
\exp (\phi (y_1, y_2,
\ldots, y_n)) = \sum_{y_1 \in \{ 0, 1 \}}
\sum_{y_2 \in \{ 0, 1 \}} \ldots
\sum_{y_n \in \{ 0, 1 \}} \exp (\phi
(y_1, y_2, \ldots, y_n)) .\)</center><p>A simple look into this expression confirms that it scales exponentially
with \(n\), undesirable if we want to model and solve large scale
combinatorial problem which requires this summation. Fortunately, some
researchers have developed a tractable solution to approximate this
summation, with the help of the gumbel distribution, and that is what
this post is about.</p><h2 id=auto-2>2<span style=margin-left:1em></span>Gumbel distribution<span style=margin-left:1em></span></h2><p>We cannot proceed without a proper introduction of gumbel distribution
[<a href=#bib-GumbelDistribution2019>2</a>]. A gumbel distribution \(\operatorname{Gumbel} (\mu,
\beta)\) has a cumulative density function:</p><center>\(\displaystyle F (x) = e^{- e^{- (x - \mu) / \beta}} .\)</center><p>The standard gumbel distribution has \(\mu = 0\) and \(\beta = 1\). In
the following, I will refer \(\operatorname{Gumbel}\) without any
parameters as the standard gumbel distribution. One of the important
properties that leads to the connection between maximization and
summation is as follows:</p><p><p><b>Theorem <class style="font-style: normal">1</class>. </b><i><center>\(\displaystyle \log (Z) = \log \left( \sum_{y \in Y} \exp (\phi
(y)) \right) =\mathbb{E}_{g
(y) \sim \operatorname{Gumbel}} [\max_{y
\in Y} \{ \phi (y) + g (y) \}] -
\gamma .\)</center></i></p><p><i><p>\(g (y) \sim \operatorname{Gumbel}\), with a size of \(| Y |\), is
indentically, independently distributed (i.i.d.) according to
\(\operatorname{Gumbel}\) and \(\gamma\) is the Euler-Mascheroni
cosntant [<a href=#bib-EulerMascheroniConstant2020>1</a>].</p><p><b>Proof.</b></p><p>Let \(T = \max_{y \in Y} \{ \phi (y) + g (y) \}\),</p><center>\begin{eqnarray*}
\Pr (T \leqslant t) & = & \Pr
          (\operatorname{all}y \in Y, \phi (y) + g (y)
\leqslant t)\\
& =
          & \Pr (\operatorname{all}y \in Y, g (y) \leqslant t - \phi (y))\\
& = & \prod_{y \in Y} \Pr (g (y) \leqslant t - \phi (y))\\
& = &
          \prod_{y \in Y} F (t - \phi (y)) = \prod_{y \in Y} e^{- e (- (t -
\phi (y)))}\\
& = & \exp \left( - \sum_{y \in Y} e^{(- (t - \phi
(y)))} \right)^{_{}} =
\exp \left( - \sum_{y \in Y} e^{- t} \cdot
e^{\phi (y)} \right)\\
& = & \exp \left( - e^{- t} \cdot \sum_{y
\in Y} \exp (\phi (y)) \right) =
\exp (- e^{- t} \cdot Z) = \exp
(- e^{- t} \cdot e^{\log (Z)})\\
& = & \exp (- e^{- (t - \log
      (Z))})
\end{eqnarray*}</center><p>Hence \(T \sim \operatorname{Gumbel} (\log (Z), 1)\), whose
expectation \(\mathbb{E} (T) = \log (Z) + \gamma\).</p></i></p><p><i><center>\(\displaystyle \mathbb{E}_{g (y) \sim \operatorname{Gumbel}}
[\max_{y \in Y} \{ \phi (y) + g
(y) \}] = \log (Z) + \gamma .\)</center></i></p></p><p></p><p>This theorem, while interesting, does not give us the practical
algorithm to calculate the sum. To estimate this expectation, we still
need the same amount of gumbel variables as cartesian space \(Y\).
Luckily, as proposed in [<a href=#bib-hazanPartitionFunctionRandom>3</a>], we have an alternative form to
compute the expectation.</p><p><p><b>Theorem <class style="font-style: normal">2</class>. </b><i><a id=theorem:2></a>Let \(\{ g_i
(y_i) \}_{y_i \in Y_i} \sim \operatorname{Gumbel}, i = 1, 2, \ldots,
n\), i.i.d. Then</i></p><p><i><center>\(\displaystyle \log (Z) =\mathbb{E}_{g (y_1)} \left[ \max_{y_1}
\mathbb{E}_{g (y_2)} \left[
\max_{y_2} \cdots \mathbb{E}_{g (y_n)}
\left[ \max_{y_n} \left\{ \phi (y_1,
y_2, \ldots, y_n) + \sum_{i =
1}^n g (y_i) \right\} \right] \ldots \right]
\right] - n \gamma .\)</center><p><b>Proof.</b></p><p>To give out some intuition of the proof, we can take some baby
steps.</p><p>Given any \(y_1 \in Y_1, y_2 \in Y_2, \ldots, y_{n - 1} \in Y_{n -
1}\), we take the sum over \(y_n \in Y_n\),</p><center>\(\displaystyle \log \left( \sum_{y_n} \exp (\phi (y_1, y_2, \ldots,
y_n)) \right)
=\mathbb{E}_{g (y_n) \sim \operatorname{Gumbel}}
[\max_{y_n} \{ \phi (y_1,
y_2, \ldots, y_n) + g (y_n) \}] - \gamma
.\)</center><p>For convenience, denote</p><center>\(\displaystyle T (y_1, y_2, \ldots, y_{n - 1}) = \log \left(
\sum_{y_n} \exp (\phi (y_1, y_2,
\ldots, y_n)) \right)
=\mathbb{E}_{g (y_n) \sim \operatorname{Gumbel}}
[\max_{y_n} \{
\phi (y_1, y_2, \ldots, y_n) + g (y_n) \}] - \gamma .\)</center><p>Likewise, given \(y_1 \in Y_1, y_2 \in Y_2, \ldots, y_{n - 2} \in
Y_{n - 2}\), we have,</p><center>\begin{eqnarray*}
\log \left( \sum_{y_{n - 1}} \sum_{y_n} \exp
(\phi (y_1, y_2, \ldots, y_n))
\right) & = & \log \left(
          \sum_{y_{n - 1}} \exp \left( \log \left(
\sum_{y_n} \exp (\phi
(y_1, y_2, \ldots, y_n)) \right) \right) \right)\\
& = & \log
          \left( \sum_{y_{n - 1}} \exp (T (y_1, y_2, \ldots, y_{n - 1}))
\right)\\
& = & \mathbb{E}_{g (y_{n - 1}) \sim
\operatorname{Gumbel}} [\max_{y_{n -
1}} \{ T (y_1, y_2, \ldots,
y_{n - 1}) + g (y_{n - 1}) \}] - \gamma\\
& = & \mathbb{E}_{g
          (y_{n - 1}) \sim \operatorname{Gumbel}} [\max_{y_{n -
1}} \{
\mathbb{E}_{g (y_n) \sim \operatorname{Gumbel}} [\max_{y_n} \{ \phi
(y_1, y_2, \ldots, y_n) + g (y_n) + g (y_{n - 1}) \} \}] - 2
\gamma
\end{eqnarray*}</center><p>Wash, rinse, repeat.</p><p></p><p>Now formally, like above, denote</p><center>\(\displaystyle T (y_1, y_2, \ldots, y_k) =\mathbb{E}_{g (y_{k +
1})} \left[ \max_{y_{k + 1}}
\mathbb{E}_{g (y_{k + 2})} \left[
\max_{y_{k + 2}} \cdots \mathbb{E}_{g (y_n)}
\left[ \max_{y_n}
\left\{ \phi (y_1, y_2, \ldots, y_n) + \sum_{i = k + 1}^n g
(y_i)
\right\} \right] \ldots \right] \right] - (n - k) \gamma .\)</center><p>We assume,</p><center>\(\displaystyle T (y_1, y_2, \ldots, y_k) = \log \left( \sum_{y_{k +
1} \in Y_{k + 1}, \ldots,
y_n \in Y_n} \exp (\phi (y_1, y_2, \ldots,
y_n)) \right),\)</center><p>We know from above that when \(k = n - 1\), this is true.</p><p>As for its next recursion, we need to prove that,</p><center>\begin{eqnarray*}
T (y_1, y_2, \ldots, y_{k - 1}) & = & \log
          \left( \sum_{y_k \in Y_k, \ldots,
y_n \in Y_n} \exp (\phi (y_1,
y_2, \ldots, y_n)) \right)\\
& = & \log \left( \sum_{y_k \in Y_k}
\exp \left( \log \left( \sum_{y_{k +
1} \in Y_{k + 1}, \ldots, y_n
\in Y_n} \exp (\phi (y_1, y_2, \ldots, y_n))
\right) \right)
\right)\\
& = & \log \left( \sum_{y_k \in Y_k} \exp (T (y_1, y_2,
\ldots, y_k))
\right)\\
& = & \mathbb{E}_{g (y_k) \sim
    \operatorname{Gumbel}} [\max_{y_k} \{ T
(y_1, y_2, \ldots, y_k) +
g (y_k) \}] - \gamma\\
& = & \mathbb{E}_{g (y_k)} \left[
 \max_{y_k} \ldots \mathbb{E}_{g (y_n)}
\left[ \max_{y_n} \left\{
\phi (y_1, y_2, \ldots, y_n) + \sum_{i = k}^n g
(y_i) \right\}
\right] - (n - k + 1) \gamma \right]
\end{eqnarray*}</center><p>With induction,</p></i></p><p><i><center>\(\displaystyle T (y_0) = \log (Z) =\mathbb{E}_{g (y_1)} \left[
\max_{y_1} \mathbb{E}_{g
(y_2)} \left[ \max_{y_2} \cdots
\mathbb{E}_{g (y_n)} \left[ \max_{y_n} \left\{
\phi (y_1, y_2,
\ldots, y_n) + \sum_{i = 1}^n g (y_i) \right\} \right]
\ldots
\right] \right] - n \gamma .\)</center></i></p></p><p>With this Theorem <a href=#theorem:2>2</a>, we only need to estimate \(n\) random
gumbel variables.</p><h2 id=auto-3>3<span style=margin-left:1em></span>Personal remarks<span style=margin-left:1em></span></h2><p>This blog is nearly two months overdue, in my opinion, there are two
reasons for it. First of all, I think this article does not bring much
knowledge to the table, a &ldquo;rip-off&rdquo; from other people's
work. Therefore, I am reluctant to publish this article. Second of all,
it takes away huge chunks of my time to write (borrow) something
worthwhile, and I am always busy even though sometimes I don't know why
I am so busy :). Thus, I am also reluctant to spend time writing up
&ldquo;rip-off&rdquo; articles such as this. It takes great efforts to
present knowledge in an accessible manner, which I do not claim this
article is. In light of such circumstances and some advice taken from <a href=http://matt.might.net/articles/how-to-blog-as-an-academic/>Matt Might on low-cost blogging</a>, I may have to change my
positioning of this blog: from writing up my original thinkings to
borrowing knowledge from other sources or even just a byproduct of
reading some papers along with a little bit of my interpretation. The
main purpose of this blog is to train me to engage in academic writings
as well as correcting my mental models on various problems and
solutions. It may not be as rigorous and complete when first published,
but it will be refined over time. In this way, I may find it more
approachable.</p><h2 id=auto-4>Bibliography<span style=margin-left:1em></span></h2><div style=text-indent:0><div class=compact-block><font style="font-size: 84.0%"><dl><p><p><strong>[1]</strong> <a id=bib-EulerMascheroniConstant2020></a>Euler&ndash;Mascheroni
constant. Page Version ID: 943784573.</p><p><strong>[2]</strong> <a id=bib-GumbelDistribution2019></a>Gumbel distribution. Page
Version ID: 930259942.</p><p><strong>[3]</strong> <a id=bib-hazanPartitionFunctionRandom></a>Tamir Hazan and Tommi
Jaakkola. On the Partition Function and Random Maximum
A-Posteriori Perturbations. Page 8.</p></p></dl></font></div></div></body></html></div></div></section><footer class=footer><div class="content has-text-centered"><p><a href=https://github.com/orf/bare-hugo-theme target=_blank>Bare Hugo theme.</a></p></div></footer><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','Ua-139857559-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>